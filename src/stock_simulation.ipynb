{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb06b552",
   "metadata": {},
   "source": [
    "# VaR Backtesting of Simulated Stock Prices\n",
    "### Billy Ford\n",
    "## Introduction\n",
    "In this notebook we demonstrate some of the basic concepts behind traded risk modelling, by performing a Monte-Carlo simulation of Googles stock price, measure the Value-at-Risk, and finally backtesting our model.\n",
    "\n",
    "## Geometric Brownian motion\n",
    "We model the stock price $S_t$ using a Geometric Brownian Motion process, which takes the form:\n",
    "\n",
    "$$ dS_t = \\mu S_t dt + \\sigma S_t dW_t, $$\n",
    "\n",
    "where $\\mu$ is the model drift, $\\sigma$ is the volatility, and $dW$ represents a Wiener process which encodes the stochastic fluctuation of the stock price. The solution for the time evolution of $S$ is given by:\n",
    "\n",
    "$$ S_t = S_0 \\exp\\left(\\left(\\mu - \\frac{\\sigma^2}{2}\\right) t - \\sigma W_t \\right). $$\n",
    "\n",
    "## Preliminaries\n",
    "We begin by loading in the classes and functions defined to contain the data and models, as well as for performing the risk calculations and backtesting. We also define the parameters for our calibration and simulation, as well as specifying the stock we wish to simulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23802fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_classes import *\n",
    "from risk_calculations import *\n",
    "from backtest_functions import *\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0829000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set calibrations and simulation params\n",
    "stock_name = \"GOOG\"\n",
    "calibration_start_date = \"01/01/2015\"\n",
    "calibration_end_date = \"31/12/2020\"\n",
    "sim_start_date = \"01/01/2021\"\n",
    "sim_end_date = \"01/12/2022\"\n",
    "npaths = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b7ce3da",
   "metadata": {},
   "source": [
    "## A Glimpse at the Data\n",
    "First let us plot the historical time series to be used to calibrate the GBM model. We can see the steady and then more dramatic rise in the 5 year calibration period. We should therefore expect to observe an positive drift in the model calibration process. \n",
    "\n",
    "We can also look at the daily returns of the stock price. We observe the returns resemble a normal distribution around zero. As a sanity check, we can sum the returns to confirm the total is the difference between the current price and intial price on the calibration start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d898ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'yfinance' has no attribute 'pdry_override'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\333891\\Documents\\backtesting_demo\\RiskSim\\src\\stock_classes.py:37\u001b[0m, in \u001b[0;36mStockData.get_data\u001b[1;34m(self, start_date, end_date)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m \tdata \u001b[39m=\u001b[39m pdr\u001b[39m.\u001b[39;49mDataReader(ticker, data_source \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39myahoo\u001b[39;49m\u001b[39m'\u001b[39;49m, \\\n\u001b[0;32m     38\u001b[0m \t   \tstart \u001b[39m=\u001b[39;49m start_date, end \u001b[39m=\u001b[39;49m end_date)\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\risksim_env\\Lib\\site-packages\\pandas\\util\\_decorators.py:213\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\risksim_env\\Lib\\site-packages\\pandas_datareader\\data.py:379\u001b[0m, in \u001b[0;36mDataReader\u001b[1;34m(name, data_source, start, end, retry_count, pause, session, api_key)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mif\u001b[39;00m data_source \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39myahoo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m YahooDailyReader(\n\u001b[0;32m    371\u001b[0m         symbols\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    372\u001b[0m         start\u001b[39m=\u001b[39;49mstart,\n\u001b[0;32m    373\u001b[0m         end\u001b[39m=\u001b[39;49mend,\n\u001b[0;32m    374\u001b[0m         adjust_price\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    375\u001b[0m         chunksize\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[0;32m    376\u001b[0m         retry_count\u001b[39m=\u001b[39;49mretry_count,\n\u001b[0;32m    377\u001b[0m         pause\u001b[39m=\u001b[39;49mpause,\n\u001b[0;32m    378\u001b[0m         session\u001b[39m=\u001b[39;49msession,\n\u001b[1;32m--> 379\u001b[0m     )\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    381\u001b[0m \u001b[39melif\u001b[39;00m data_source \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miex\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\risksim_env\\Lib\\site-packages\\pandas_datareader\\base.py:253\u001b[0m, in \u001b[0;36m_DailyBaseReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msymbols, (string_types, \u001b[39mint\u001b[39m)):\n\u001b[1;32m--> 253\u001b[0m     df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_one_data(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_params(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msymbols))\n\u001b[0;32m    254\u001b[0m \u001b[39m# Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\risksim_env\\Lib\\site-packages\\pandas_datareader\\yahoo\\daily.py:153\u001b[0m, in \u001b[0;36mYahooDailyReader._read_one_data\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    152\u001b[0m     j \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(re\u001b[39m.\u001b[39msearch(ptrn, resp\u001b[39m.\u001b[39mtext, re\u001b[39m.\u001b[39mDOTALL)\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m))\n\u001b[1;32m--> 153\u001b[0m     data \u001b[39m=\u001b[39m j[\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mdispatcher\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mstores\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mHistoricalPriceStore\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Read and plot google stock data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m google_data \u001b[39m=\u001b[39m StockData(stock_name)\n\u001b[1;32m----> 3\u001b[0m google_data\u001b[39m.\u001b[39;49mget_data(calibration_start_date, calibration_end_date)\n\u001b[0;32m      4\u001b[0m google_data\u001b[39m.\u001b[39mtimeseries_plot()\n\u001b[0;32m      5\u001b[0m google_data\u001b[39m.\u001b[39mreturns_plot()\n",
      "File \u001b[1;32mc:\\Users\\333891\\Documents\\backtesting_demo\\RiskSim\\src\\stock_classes.py:40\u001b[0m, in \u001b[0;36mStockData.get_data\u001b[1;34m(self, start_date, end_date)\u001b[0m\n\u001b[0;32m     37\u001b[0m \tdata \u001b[39m=\u001b[39m pdr\u001b[39m.\u001b[39mDataReader(ticker, data_source \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39myahoo\u001b[39m\u001b[39m'\u001b[39m, \\\n\u001b[0;32m     38\u001b[0m \t   \tstart \u001b[39m=\u001b[39m start_date, end \u001b[39m=\u001b[39m end_date)\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m \tyfin\u001b[39m.\u001b[39;49mpdry_override()\n\u001b[0;32m     41\u001b[0m \tdata \u001b[39m=\u001b[39m pdr\u001b[39m.\u001b[39mget_data_yahoo(ticker, start \u001b[39m=\u001b[39m start_date, end \u001b[39m=\u001b[39m end_date)\n\u001b[0;32m     42\u001b[0m data \u001b[39m=\u001b[39m data[[\u001b[39m\"\u001b[39m\u001b[39mAdj Close\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'yfinance' has no attribute 'pdry_override'"
     ]
    }
   ],
   "source": [
    "# Read and plot google stock data\n",
    "google_data = StockData(stock_name)\n",
    "google_data.get_data(calibration_start_date, calibration_end_date)\n",
    "google_data.timeseries_plot()\n",
    "google_data.returns_plot()\n",
    "\n",
    "# Sum daily returns\n",
    "tot_returns = google_data.data[\"Returns\"].sum()\n",
    "price_diff = google_data.data[\"Adj Close\"][-1] - google_data.data[\"Adj Close\"][0]\n",
    "print(\"Total returns: \"+str(tot_returns))\n",
    "print(\"Final price - Start price: \"+str(price_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12701e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(google_data.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3537ad2a",
   "metadata": {},
   "source": [
    "## Calibrating the Model\n",
    "In this step we use historical data to calibrate the GBM model. This is simply a way of extracting the drift and volatility model parameters for the GBM simulation, $\\mu$ and $\\sigma$. To do this, we start with the daily absolute returns, and convert to a log returns:\n",
    "\n",
    "$$ R^{\\text{log}}_{t} = \\log \\left(1 + \\frac{R^{\\text{abs}}_t}{ R^{\\text{abs}}_{t-1}}\\right).$$\n",
    "\n",
    "We compute the mean ($\\mu$) and standard deviation ($\\sigma$) of this distribution, and then set the simulation model parameters for drift and volatility as $\\left(\\mu - \\frac{\\sigma^2}{2} \\right)$ and $\\sigma$ respectively.\n",
    "\n",
    "We can take a look at the string representation of the model object, showing now that calibration has been performed using the data we plotted earlier, and the calibration parameters obtained. However there is one last step, to simulate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf09553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model and calibrate parameters using historical data\n",
    "google_model = StockModel(stock_name)\n",
    "google_model.calibrate(calibration_start_date, calibration_end_date)\n",
    "print(google_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0bbb410",
   "metadata": {},
   "source": [
    "## Simulating the Model\n",
    "Finally, we take the calibrated model and perform and a Monte Carlo simulation to observe the projected returns across a number of simulated paths. Starting with a day on which we can observe the price, which we refer to as $S_0$, the price on the next day will be:\n",
    "\n",
    "$$ S_t = S_0 e^{\\mu + \\sigma W_t},$$\n",
    "\n",
    "where $W_t$ is a draw from a Wiener process. As we can see, the returns for a given day therefore depend on the price the previous day. Generally when simulating stock prices, we hope to generate a large number of paths (~$10^5$), and so generating can lead to some computational burden.\n",
    "\n",
    "Rather than explicitely looping over all days and all paths, we can generate our grid of random numbers all in one go, in an array of dimension $npaths \\times ndays$. Initialising all paths with the value of $S_0$, we can loop over the number of days in the simulation, updating all paths at the same time for each day.\n",
    "\n",
    "Simulating 1000 paths, we can see the projections. In particular, we notice that more paths lead to an increased stock price, which is expected given we calibrate the model on data from a period where the price increased, and hence yielded a positive drift.\n",
    "\n",
    "As a final sanity check, we can see the string representation of the model now shows as simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate stock price\n",
    "google_model.simulate(sim_start_date, sim_end_date, npaths)\n",
    "google_model.plot_paths()\n",
    "print(google_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7401cc2b",
   "metadata": {},
   "source": [
    "## Value-at-Risk\n",
    "In this next section we use make use of the risk_calculations.py, to compute various risk measures. The most common is Value-at-Risk, which is simply a measure of the $p\\%$ percentile of the distribution of projected PnL over an $n$-day period. A common measure is the $99\\%$ 10-day VaR, used to compute capital requirements under the Basel framework. While one can indeed directly measure the PnL over an arbitrary $n$-day horizon, it is commonplace to simply measure the 1-day VaR and use the assumption\n",
    "\n",
    "$$ VaR_{n-\\text{day}} \\simeq VaR_{1-\\text{day}} \\times \\sqrt{n}.$$\n",
    "\n",
    "We set up our VaR parameters as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set VaR parameters\n",
    "var_start_date = \"01/01/2020\"\n",
    "var_end_date = \"01/01/2022\"\n",
    "horizon = 10\n",
    "percentile = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data and compute VaR\n",
    "var_data = grab_hist_data(stock_name, var_start_date, var_end_date)\n",
    "var = var(stock_name, var_start_date, var_end_date, horizon, percentile)\n",
    "print(str(horizon)+\"-day VaR value: \"+str(var))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5ea192b",
   "metadata": {},
   "source": [
    "## Expected shortfall\n",
    "There are a number of drawbacks in VaR, for example it does not encod information on what lies beyond the tail, only where the tail is. It is also not additive, so can be difficult to aggregate across trades in a portfolio. Indeed with this in mind, new banking regulations known as the fundamental review of the trading book (FRTB) are being introduced, in which VaR is replaced with a new metric - expected shortfall (ES).\n",
    "\n",
    "ES is similar to VaR, it is a percentile calculation applied to a returns distribution, however instead of simnoly measuring the percentile, ES is calculated by averaging the returns beyond the specified percentile $p$,\n",
    "\n",
    "$$ ES_p = -\\frac{1}{p} \\int_0^p VaR_\\gamma d\\gamma. $$\n",
    "\n",
    "We compute the ES below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ba041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute expected shortfall\n",
    "es = expected_shortfall(stock_name, var_start_date, var_end_date, horizon, percentile)\n",
    "print(str(horizon)+\"-day ES value: \"+str(es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7751b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot returns distribution with VaR\n",
    "plot_returns(var_data, horizon, var, es)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d40720",
   "metadata": {},
   "source": [
    "## Backtesting\n",
    "So far we used a model to simulate a stock price, and we have seen some of the risk calculations one can perform to quantify the risk of a portfolio. Next, we look at backtesting (BT), which is a way of assessing the risk model.\n",
    "\n",
    "By using historical data, we can measure how often the actual daily returns exceed our worst-case estimate (i.e. VaR or ES), and count the number of times the actual returns exceed this estimate, a.k.a a BT exception. \n",
    "\n",
    "As a demonstration, we perform backtest on the historical VaR for google's stock price. To do so, we count the number of VaR backtesting exceptions, which are defined where the actual realised (negative) returns exceeds the VaR estimate. We use a one year VaR window, and backtest over a two year period between January 2020 to January 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3862ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define backtest parameters\n",
    "bt_start_date = \"01/01/2020\"\n",
    "bt_end_date = \"01/01/2022\"\n",
    "n_years = 1\n",
    "percentile = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140778f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of exceptions\n",
    "n_observations = ((pd.to_datetime(bt_end_date, format='%d/%m/%Y')\\\n",
    "                                     - pd.to_datetime(bt_start_date, format='%d/%m/%Y')).days)\n",
    "n_exceptions = var_exceptions_count(stock_name, bt_start_date, bt_end_date, n_years, percentile)\n",
    "print(\"Number of observations: \"+str(n_observations))\n",
    "print(\"Number of exceptions: \"+str(n_exceptions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b72d106",
   "metadata": {},
   "source": [
    "## Interpreting the exception count\n",
    "We have created a simple historical simulation VaR model to estimate the one day 95% tail loss, and counted how often this estimate is breached. But how do we make sense of this result? This is where statistical testing enters.\n",
    "\n",
    "In market risk, as we are projecting daily returns, each daily return is an independent observation, with a fixed probability of an exception occuring. We would therefore expect the number of exceptions to follow a binomial distribution, similar to fipping a coin. For $n$ trial of a process with two outcomes with probabilities $p$ and $1 = 1-p$, the probability of $x$ outcomes is: \n",
    "\n",
    "$$ P_x = \\begin{pmatrix} n \\\\ x \\end{pmatrix} p^x q^{n-x}.$$\n",
    "\n",
    "In our example using two years of data, we have 731 independent observations, and under the assumption that our VaR model is suitable, we therefore have a 5% probability each day of an exception (using the 95% VaR). Using the above, we can therefore construct a distribution of the probability of obtaining $x$ backtesting exceptions. \n",
    "\n",
    "The corresponding p-value for the model represents the probability of obtaining a test result (number of exceptions) at least as extreme than the value observed. This is obtained by evaluating the observed number of exceptions against the cumulative distribution function of the binomial distribution. \n",
    "\n",
    "In other words, if we say our null hypothesis is that the model used to compute a VaR value is an appropriate model that matches the real-life stock moves, the $p$-value gives the probability of accepting the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute binomial distribution and generate p-value\n",
    "p_value = compute_p_value(n_observations, n_exceptions, float(1-percentile/100), plots=True)\n",
    "print(\"p-Value: \"+str(p_value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c9fa048",
   "metadata": {},
   "source": [
    "We observe a large $p$-value, as the observed number of backtesting exceptions falls far to the left of the CDF (which corresponds to $1 - p\\text{-value}$). This indicates that the observed number of backtesting exceptions is not an extreme result, and that we accept our null hypothesis that the historical simulation model using a rolling two year window of time series data is an appropriate estimator of the 95$%$ 1-day VaR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0f21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
